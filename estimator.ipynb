{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ye221nv8bY7d"
      },
      "outputs": [],
      "source": [
        "def estimate_memory_usage(model_size, precision, sequence_length, batch_size, hidden_size, num_layers, optimizer):\n",
        "    \"\"\"\n",
        "    Estimate the VRAM required for fine-tuning a model.\n",
        "\n",
        "    Parameters:\n",
        "        model_size (int): Number of parameters in the model (e.g., 7 billion = 7e9).\n",
        "        precision (int): Bytes per parameter (e.g., 2 for FP16, 4 for FP32).\n",
        "        sequence_length (int): Length of input sequences.\n",
        "        batch_size (int): Number of samples per batch.\n",
        "        hidden_size (int): Hidden size of the model.\n",
        "        num_layers (int): Number of layers in the model.\n",
        "        optimizer (str): Optimizer type ('adam', 'adamw', 'sgd', 'rmsprop', 'adafactor').\n",
        "\n",
        "    Returns:\n",
        "        float: Estimated VRAM usage in GB.\n",
        "    \"\"\"\n",
        "    # Define optimizer memory factors\n",
        "    optimizer_memory_factors = {\n",
        "        'adam': 2,\n",
        "        'adamw': 2,\n",
        "        'sgd': 1,\n",
        "        'rmsprop': 2,\n",
        "        'adafactor': 1.5\n",
        "    }\n",
        "\n",
        "    optimizer = optimizer.lower()\n",
        "    if optimizer not in optimizer_memory_factors:\n",
        "        print(f\"Please specify a valid optimizer. Valid optimizers are: {list(optimizer_memory_factors.keys())}\")\n",
        "        return\n",
        "\n",
        "    # Model Weights\n",
        "    model_weight_memory = model_size * precision\n",
        "\n",
        "    # Activations (approximate)\n",
        "    activation_memory = batch_size * sequence_length * hidden_size * num_layers * precision\n",
        "\n",
        "    # Gradients (similar size to activations)\n",
        "    gradient_memory = activation_memory\n",
        "\n",
        "    # Optimizer States\n",
        "    optimizer_factor = optimizer_memory_factors[optimizer]\n",
        "    optimizer_memory = optimizer_factor * model_weight_memory\n",
        "\n",
        "    # Total Memory\n",
        "    total_memory = model_weight_memory + activation_memory + gradient_memory + optimizer_memory\n",
        "\n",
        "    # Convert to GB\n",
        "    total_memory_gb = total_memory / (1024 ** 3)\n",
        "    return total_memory_gb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEyPr0tPbY7e"
      },
      "source": [
        "### Testing the function\n",
        "The following code will test the function using the torch.cuda.memory_allocated, which tells the user how much memory will be allocated during a finetuning run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GU9f-n1-tYf5"
      },
      "outputs": [],
      "source": [
        "TEXT = \"\"\"\n",
        "Once upon a time, in a faraway kingdom, there lived a wise and kind king. His name was King Cedric, and he ruled the kingdom with fairness and compassion. The people loved him dearly for his just decisions and his love for the well-being of his subjects.\n",
        "\n",
        "One day, a messenger arrived at the palace with a troubling message. The neighboring kingdom had been struck by a terrible drought, and the people there were suffering greatly. King Cedric, being the compassionate ruler that he was, immediately called for a meeting of his advisors to discuss how they could help.\n",
        "\n",
        "The advisors were divided. Some suggested sending food and water to the neighboring kingdom, while others suggested sending gold to help them rebuild their infrastructure. King Cedric listened attentively to all of their ideas, weighing the pros and cons of each suggestion.\n",
        "\n",
        "After much discussion, King Cedric made his decision. He would send not only food and water but also skilled engineers and laborers to help rebuild the kingdom's infrastructure. He knew that this would not only help the people in the neighboring kingdom but also strengthen the bond between the two nations.\n",
        "\n",
        "As the days passed, King Cedric's decision proved to be a wise one. The neighboring kingdom flourished, and the bond between the two kingdoms grew stronger. King Cedric's people admired him even more for his selflessness and the positive impact he had on the world around him.\n",
        "\n",
        "And so, the story of King Cedric and his wise decision became a tale told for generations, reminding everyone of the power of compassion, leadership, and the importance of helping others in times of need.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ByqMqXwbY7f",
        "outputId": "ce3426c4-e550-4605-b824-56b905cf9a96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Allocated VRAM over 10 passes: 2.63 GB\n",
            "Average Peak VRAM over 10 passes: 8.77 GB\n",
            "Variance of Allocated VRAM: 0.0000 GB^2\n",
            "Variance of Peak VRAM: 0.1711 GB^2\n",
            "\n",
            "Estimated VRAM: 13.38 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
        "import numpy as np\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Move Model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Tokenize the long text document\n",
        "batch_size = 4  # Adjust batch size as needed\n",
        "sequence_length = model.config.max_position_embeddings\n",
        "\n",
        "# Tokenizing and preparing input text for the model\n",
        "long_text_input = tokenizer([TEXT] * batch_size,\n",
        "                            return_tensors=\"pt\",\n",
        "                            padding=\"max_length\",\n",
        "                            max_length=sequence_length,\n",
        "                            truncation=True).to(device)\n",
        "\n",
        "# Resize model's token embeddings with new special tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Reset GPU Memory Stats\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Measure VRAM During Multiple Training Steps\n",
        "def measure_vram(num_passes=10):\n",
        "    allocated_memory_list = []\n",
        "    peak_memory_list = []\n",
        "\n",
        "    for i in range(num_passes):\n",
        "        # Forward Pass\n",
        "        labels = long_text_input['input_ids']\n",
        "        outputs = model(**long_text_input, labels=labels)\n",
        "        loss = outputs.loss  # Use the proper loss attribute\n",
        "\n",
        "        # Backward Pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Memory Stats\n",
        "        allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # GB\n",
        "        peak_memory = torch.cuda.max_memory_allocated(device) / 1024**3  # GB\n",
        "\n",
        "        allocated_memory_list.append(allocated_memory)\n",
        "        peak_memory_list.append(peak_memory)\n",
        "\n",
        "    # Return average memory stats over all passes\n",
        "    avg_allocated_memory = sum(allocated_memory_list) / num_passes\n",
        "    avg_peak_memory = sum(peak_memory_list) / num_passes\n",
        "\n",
        "    # Calculate variance\n",
        "    allocated_variance = np.var(allocated_memory_list)\n",
        "    peak_variance = np.var(peak_memory_list)\n",
        "\n",
        "    return avg_allocated_memory, avg_peak_memory, allocated_variance, peak_variance\n",
        "\n",
        "# Measure VRAM over 10 passes\n",
        "num_passes = 10\n",
        "avg_allocated, avg_peak, allocated_variance, peak_variance = measure_vram(num_passes=num_passes)\n",
        "print(f\"Average Allocated VRAM over {num_passes} passes: {avg_allocated:.2f} GB\")\n",
        "print(f\"Average Peak VRAM over {num_passes} passes: {avg_peak:.2f} GB\")\n",
        "print(f\"Variance of Allocated VRAM: {allocated_variance:.4f} GB^2\")\n",
        "print(f\"Variance of Peak VRAM: {peak_variance:.4f} GB^2\")\n",
        "\n",
        "# Dynamically retrieve model parameters\n",
        "hidden_size = model.config.n_embd  # Hidden size\n",
        "num_layers = model.config.n_layer  # Number of layers\n",
        "\n",
        "# Automatically detect precision (FP16 if available)\n",
        "# precision = 16 if model.device.type == \"cuda\" and torch.cuda.get_device_capability(model.device)[0] >= 7 else 32\n",
        "precision = 32\n",
        "\n",
        "# Total parameters in the model\n",
        "model_size = sum(p.numel() for p in model.parameters())  # Total parameters\n",
        "\n",
        "# Estimate VRAM based on the model's architecture and precision\n",
        "estimated_vram = estimate_memory_usage(\n",
        "    model_size=model_size,\n",
        "    precision=precision,\n",
        "    sequence_length=sequence_length,\n",
        "    batch_size=batch_size,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    optimizer=\"adam\"\n",
        ")\n",
        "\n",
        "print(f\"\\nEstimated VRAM: {estimated_vram:.2f} GB\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
